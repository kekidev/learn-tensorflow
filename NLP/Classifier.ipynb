{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Course 3 - Week 2 - Lesson 2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "import json\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "outputs": [],
      "metadata": {
        "id": "XYYDvoskkE61"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "vocab_size = 10000\n",
        "embedding_dim = 16\n",
        "max_length = 100\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "training_size = 20000\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "0eJSTTYnkJQd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "with open(\"datasets/sarcasm.json\", 'r') as f:\n",
        "    datastore = json.load(f)\n",
        "\n",
        "sentences = []\n",
        "labels = []\n",
        "\n",
        "for item in datastore:\n",
        "    sentences.append(item['headline'])\n",
        "    labels.append(item['is_sarcastic'])"
      ],
      "outputs": [],
      "metadata": {
        "id": "oaLaaqhNkUPd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "training_sentences = sentences[0:training_size]\n",
        "testing_sentences = sentences[training_size:]\n",
        "training_labels = labels[0:training_size]\n",
        "testing_labels = labels[training_size:]"
      ],
      "outputs": [],
      "metadata": {
        "id": "S1sD-7v0kYWk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
      ],
      "outputs": [],
      "metadata": {
        "id": "3u8UB0MCkZ5N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "# Need this block to get it to work with TensorFlow 2.x\n",
        "import numpy as np\n",
        "training_padded = np.array(training_padded)\n",
        "training_labels = np.array(training_labels)\n",
        "testing_padded = np.array(testing_padded)\n",
        "testing_labels = np.array(testing_labels)"
      ],
      "outputs": [],
      "metadata": {
        "id": "GrAlWBKf99Ya"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(24, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "outputs": [],
      "metadata": {
        "id": "FufaT4vlkiDE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "model.summary()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 16)           160000    \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 24)                408       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 25        \n",
            "=================================================================\n",
            "Total params: 160,433\n",
            "Trainable params: 160,433\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "metadata": {
        "id": "XfDt1hmYkiys"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "num_epochs = 30\n",
        "history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=2)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x16788e4c0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x16788e4c0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-07-12 23:53:50.937994: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
            "2021-07-12 23:53:50.940034: W tensorflow/core/platform/profile_utils/cpu_utils.cc:126] Failed to get CPU frequency: 0 Hz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x177a89790> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x177a89790> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "625/625 - 1s - loss: 0.6479 - accuracy: 0.6129 - val_loss: 0.5276 - val_accuracy: 0.7357\n",
            "Epoch 2/30\n",
            "625/625 - 1s - loss: 0.3962 - accuracy: 0.8448 - val_loss: 0.3789 - val_accuracy: 0.8405\n",
            "Epoch 3/30\n",
            "625/625 - 1s - loss: 0.2980 - accuracy: 0.8825 - val_loss: 0.3488 - val_accuracy: 0.8568\n",
            "Epoch 4/30\n",
            "625/625 - 1s - loss: 0.2472 - accuracy: 0.9056 - val_loss: 0.3420 - val_accuracy: 0.8571\n",
            "Epoch 5/30\n",
            "625/625 - 1s - loss: 0.2128 - accuracy: 0.9193 - val_loss: 0.3536 - val_accuracy: 0.8506\n",
            "Epoch 6/30\n",
            "625/625 - 0s - loss: 0.1866 - accuracy: 0.9311 - val_loss: 0.3662 - val_accuracy: 0.8490\n",
            "Epoch 7/30\n",
            "625/625 - 0s - loss: 0.1648 - accuracy: 0.9394 - val_loss: 0.3921 - val_accuracy: 0.8425\n",
            "Epoch 8/30\n",
            "625/625 - 0s - loss: 0.1486 - accuracy: 0.9449 - val_loss: 0.3864 - val_accuracy: 0.8553\n",
            "Epoch 9/30\n",
            "625/625 - 0s - loss: 0.1318 - accuracy: 0.9531 - val_loss: 0.4084 - val_accuracy: 0.8493\n",
            "Epoch 10/30\n",
            "625/625 - 0s - loss: 0.1201 - accuracy: 0.9588 - val_loss: 0.4398 - val_accuracy: 0.8435\n",
            "Epoch 11/30\n",
            "625/625 - 1s - loss: 0.1086 - accuracy: 0.9617 - val_loss: 0.4724 - val_accuracy: 0.8368\n",
            "Epoch 12/30\n",
            "625/625 - 1s - loss: 0.0988 - accuracy: 0.9672 - val_loss: 0.5002 - val_accuracy: 0.8363\n",
            "Epoch 13/30\n",
            "625/625 - 0s - loss: 0.0900 - accuracy: 0.9704 - val_loss: 0.5118 - val_accuracy: 0.8413\n",
            "Epoch 14/30\n",
            "625/625 - 0s - loss: 0.0837 - accuracy: 0.9712 - val_loss: 0.5668 - val_accuracy: 0.8314\n",
            "Epoch 15/30\n",
            "625/625 - 0s - loss: 0.0749 - accuracy: 0.9753 - val_loss: 0.5720 - val_accuracy: 0.8340\n",
            "Epoch 16/30\n",
            "625/625 - 0s - loss: 0.0681 - accuracy: 0.9786 - val_loss: 0.6104 - val_accuracy: 0.8314\n",
            "Epoch 17/30\n",
            "625/625 - 0s - loss: 0.0615 - accuracy: 0.9797 - val_loss: 0.6906 - val_accuracy: 0.8247\n",
            "Epoch 18/30\n",
            "625/625 - 1s - loss: 0.0581 - accuracy: 0.9827 - val_loss: 0.6844 - val_accuracy: 0.8264\n",
            "Epoch 19/30\n",
            "625/625 - 1s - loss: 0.0531 - accuracy: 0.9842 - val_loss: 0.7174 - val_accuracy: 0.8255\n",
            "Epoch 20/30\n",
            "625/625 - 1s - loss: 0.0479 - accuracy: 0.9853 - val_loss: 0.7667 - val_accuracy: 0.8244\n",
            "Epoch 21/30\n",
            "625/625 - 0s - loss: 0.0451 - accuracy: 0.9859 - val_loss: 0.7932 - val_accuracy: 0.8225\n",
            "Epoch 22/30\n",
            "625/625 - 0s - loss: 0.0404 - accuracy: 0.9879 - val_loss: 0.8441 - val_accuracy: 0.8205\n",
            "Epoch 23/30\n",
            "625/625 - 1s - loss: 0.0383 - accuracy: 0.9886 - val_loss: 0.8799 - val_accuracy: 0.8179\n",
            "Epoch 24/30\n",
            "625/625 - 1s - loss: 0.0345 - accuracy: 0.9897 - val_loss: 0.9258 - val_accuracy: 0.8189\n",
            "Epoch 25/30\n",
            "625/625 - 0s - loss: 0.0322 - accuracy: 0.9909 - val_loss: 0.9731 - val_accuracy: 0.8158\n",
            "Epoch 26/30\n",
            "625/625 - 0s - loss: 0.0290 - accuracy: 0.9912 - val_loss: 1.0377 - val_accuracy: 0.8141\n",
            "Epoch 27/30\n",
            "625/625 - 0s - loss: 0.0262 - accuracy: 0.9928 - val_loss: 1.0592 - val_accuracy: 0.8144\n",
            "Epoch 28/30\n",
            "625/625 - 1s - loss: 0.0252 - accuracy: 0.9929 - val_loss: 1.0835 - val_accuracy: 0.8131\n",
            "Epoch 29/30\n",
            "625/625 - 1s - loss: 0.0245 - accuracy: 0.9930 - val_loss: 1.1049 - val_accuracy: 0.8132\n",
            "Epoch 30/30\n",
            "625/625 - 1s - loss: 0.0218 - accuracy: 0.9939 - val_loss: 1.1789 - val_accuracy: 0.8129\n"
          ]
        }
      ],
      "metadata": {
        "id": "2DTKQFf1kkyc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "source": [
        "sentence = [\n",
        "\t\"granny starting to fear spiders in the garden might be real\",\n",
        "\t\"the weather today is bright and sunny\",\n",
        "\t\"he is really happy and enjoyed\",\n",
        "\t\"she is really depressed and doing a bad things\",\n",
        "]\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentence)\n",
        "\n",
        "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "print(model.predict(padded))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[9.2355901e-01]\n",
            " [5.0941577e-07]\n",
            " [2.5461966e-07]\n",
            " [3.8105008e-04]\n",
            " [3.5291246e-01]]\n"
          ]
        }
      ],
      "metadata": {}
    }
  ]
}